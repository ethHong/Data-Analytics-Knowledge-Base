{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to MkDocs","text":"<p>For full documentation visit mkdocs.org. change test</p>"},{"location":"#commands","title":"Commands","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre>"},{"location":"markdowns/Parameter_Estimataion/","title":"\ud83d\udcc4 Parameter_Estimataion","text":"<p>reference_docs: \"Regression_Evaluation, test_stat_basics\"</p> <p>Image: A spoiler for today's topic</p> <p></p> <p>In the first post of DS / ML Back to the Basics series, I would like to go over what does it mean when we do the 'modeling', and how do we evaluate if the model is good, or bad. I will use 'mean estimator' to explain statistical implications, and cover Simple Linear Regression model in the follow-up post. Maybe 'mean estimation' could be seem as very basic concept, but I will focus more on the statistical implication of estimation, and standard error.</p>"},{"location":"markdowns/Parameter_Estimataion/#what-does-is-mean-to-do-modeling","title":"What does is mean to do 'modeling?'","text":"<p>When I was an undergraduate student, I learned about a variety of machine learning models that were transforming the field. I was trained to implement these models in Python and apply them to sample datasets. It felt like magic: you write code, input training data, train the model, and it predicts the result.</p> <p>However, after working as a full-time product manager in the IT industry for three years, I discovered that in real-world business scenarios, simply applying these models rarely works as expected.</p> <p>Through my experience in ML projects in an AI lab, collaborating with data scientists as a product manager, and studying statistical analysis in depth, I now understand that this happens when predictive models are used without understanding the implications behind them. Modeling is not about building a magical pipeline that produces perfect outputs just because the model is the latest and most advanced. It's about 1) effectively predicting real-world statistical patterns, 2) using a sample dataset that ideally reflects those patterns, and 3) minimizing possible errors.</p>"},{"location":"markdowns/Parameter_Estimataion/#lets-think-of-modeling-as-estimation","title":"Let's think of modeling as 'estimation'","text":"<p>We talked a little bit about this in the previous post - MLE - Maximum Likelihood Estimator. We identify certain patterns from observed phenomena. While we cannot be certain where these patterns originate or what causes them, if we can estimate the patterns accurately, it may be possible to make valid predictions about unseen data.</p>"},{"location":"markdowns/Parameter_Estimataion/#example-of-mean-prediction","title":"Example of 'mean' prediction.","text":"<p>We learned a lot about sample means and population means in statistics class\u2014even back in high school. Let's think of the sample mean in terms of \"estimation.\" For example, let\u2019s say we want to answer the question, \"How heavy are elephants?\" This might be a difficult question because individual elephants vary in weight. However, we definitely know that elephants are much heavier than dogs or cats.</p> <p>The best way to describe \"on average, how heavy are elephants?\" would be to take the mean weight of all elephants. Since it\u2019s impossible to measure the entire population, we use a sample. Here, we can say the \"sample mean\" is an estimator for the \"population mean\"\u2014implying that the \"population mean\" is the actual (or ideal) real-world pattern describing the average weight of elephants, and we are estimating it through the \"sample mean.\"</p> <p>Based on what we learned in statistics classes, assuming that weight is normally distributed, we can say:</p> <ul> <li>Random variable for the weight of elephants: \\(Y \\sim N(\\mu, \\sigma)\\)</li> <li>Sample variable follows: \\(\\bar{Y}, s_Y\\)</li> </ul> <p>Here, \\(\\mu\\) is the population mean (i.e., the actual average weight of elephants as determined by some real-world pattern that we cannot observe directly), while \\(\\bar{Y}\\) is an estimator for what we\u2019re interested in.</p> <p>So, what we have done here is to estimate the statistic of a real-world pattern using a limited sample dataset. This is what we do for every \"estimation\" or modeling exercise. But we still have one question\u2014 Can we just accept this estimator? How precise is it?</p>"},{"location":"markdowns/Parameter_Estimataion/#evaluating-estimation","title":"Evaluating estimation.","text":"<p>What should we examine to assess the precision of an estimation? Let\u2019s think of it this way: look at the two cases below. Both cases have the same mean; however, the data in the second case is much more spread out, meaning it has a higher variance from the mean. </p> <p></p> <p>This implies that if the standard deviation is high, data points are more likely to be far from our estimated mean. (In a standard normal distribution, approximately 95% of data falls within 2 * std of the mean.) Therefore, if the standard deviation of our estimating value is low, we can say our prediction is more efficient in terms of predicting unseen future data!</p>"},{"location":"markdowns/Parameter_Estimataion/#the-how-is-standard-deviation-of-mean-estimator-like-true-implication-of-clt","title":"The, how is standard deviation of mean estimator like? - True implication of CLT","text":"<p>Let\u2019s go back to statistics class. Now it\u2019s time for the Central Limit Theorem to do its job. Assuming \\(Y\\) is normally distributed, the Central Limit Theorem states that \\(\\bar{Y} \\sim N\\left(\\mu, \\frac{\\sigma^2}{n}\\right)\\)</p> <p>if the sample size \\(n\\) is large enough. Here, we see that the variance (or standard deviation) of our mean estimator is 1) determined by the population variance, and 2) divided by sample size. What are the implications of this?</p> <ol> <li>With a larger sample size, our estimator (sample mean) will approximate to the population mean.</li> <li>Standard deviation (actually, standard error - I'll explain this later) will also diminish, resulting in more precise estimation.</li> <li>However, if the population standard deviation itself is large\u2014meaning if \\(\\sigma\\) is large\u2014there will be limitations to the precision of mean estimation. </li> </ol> <p>These implications are more apparent if we consider it this way: as our sample size approaches the population size, we are effectively sampling the entire population. Therefore, as the sample size increases, the sample mean approaches the population mean. This is why we need large amounts of \"BIG\" data.</p> <p>However, this doesn\u2019t mean our estimation will always be precise just because we use a large dataset. What if the \"true pattern\" of the real world has a huge variance? For example, what if what we\u2019re trying to estimate is not elephant weight, but the \"income of people living in California?\" California has a large population with a wide range of incomes, so even if we take the population mean, it may not adequately represent a \"pattern.\" Therefore, for this type of prediction problem, using a sample mean predictor would not work well, even if we take a large sample from California.</p>"},{"location":"markdowns/Parameter_Estimataion/#why-does-dividing-segment-work-better-when-estimating-statistics","title":"Why does 'dividing segment' work better when estimating statistics?","text":"<p>So, what can we do in these kinds of cases? Product managers or analysts might suggest \"segmenting the data.\" The reason this approach works is that by dividing segments, we can reduce the population variance.</p> <p>If we divide the \"population\" into different segments\u2014e.g., by age group\u2014it\u2019s more likely that people within the same age group will have a similar range of income. By defining segmented populations, we can reduce the variance of the population statistic. Although the maximum sample size is reduced (since the population is now divided into segments), this segmentation allows us to make better predictions by effectively narrowing down the population variance.</p> <p>Let\u2019s try this with a data example. There\u2019s an older dataset that contains wage data along with demographic information\u2014the ISLR Wage dataset\u2014which we\u2019ll analyze using R.</p> <pre><code>install.packages(\"ISLR\", repos = \"http://cran.us.r-project.org\")\n\nlibrary(ISLR)\n\n# Load and view Wage dataset\ndata(\"Wage\")\nhead(Wage)\n</code></pre> <p></p> <p>This is how the data looks like. Let's just take data from 2009 (since this is the most recent year from this dataset), and divide age group.</p> <pre><code>df &lt;- Wage[Wage$year == max(Wage[, 'year']), ]\ndf$age_group &lt;- ifelse(df$age &lt; 20, \"Immature\",\n                       ifelse(df$age &lt;= 30, \"Twenties\",\n                              ifelse(df$age &lt;= 60, \"Middle aged\",\n                                     \"Senior\")))\ndf &lt;- df[, c('age_group', 'wage')]\nhead(df)\n</code></pre> <p>Now, let's compare total variance, and variaces for each age groups!</p> <pre><code>hist(df$wage, main = paste(\"All age wage - variance:\", round(var(df$wage), 2)), \n     xlab = \"Wage\", ylab = \"Frequency\")\n# Immature\nimmature_age = df[df$age_group == \"Immature\", ]$wage\nhist(immature_age, main = paste(\"Under 20 wage - variance:\", round(var(immature_age), 2)), \n     xlab = \"Wage\", ylab = \"Frequency\")\n\n#Twenties\ntwenties_age = df[df$age_group == \"Twenties\", ]$wage\nhist(immature_age, main = paste(\"20~20 wage - variance:\", round(var(twenties_age), 2)), \n     xlab = \"Wage\", ylab = \"Frequency\")\n\n#Middle aged\nmiddle_age = df[df$age_group == \"Middle aged\", ]$wage\nhist(middle_age, main = paste(\"30~59 - variance:\", round(var(middle_age), 2)), \n     xlab = \"Wage\", ylab = \"Frequency\")\n</code></pre> <p></p> <p></p> <p></p> <p></p> <p>We can see that the variance for the total population was 1811\u2014a relatively large value. However, by splitting age groups, we could reduce the variance significantly for the under-20 and 20s age groups. If we use the entire population for mean estimation, \\(\\sigma_Y\\), the population standard deviation of wage, is \\(\\sqrt{\\sigma_Y} = 42.5629\\). In comparison, the standard deviation of the 20s age group segment is \\(\\sqrt{\\sigma_{Y=20}} = 29.32\\).</p> <p>In a normal distribution, 95% of data is located within \\(\\text{mean} \\pm 2\\sigma\\) (the confidence interval). Therefore, if we use the mean as the estimator, the 95% confidence interval in the first case is about 85, while in the second case, it\u2019s much narrower\u2014around 58.</p> <p>Interestingly, the middle-aged group has a much larger variance than the total population. This suggests that we selected an inappropriate segment for mean prediction. This makes sense, as the 30\u201359 age range is quite broad for generalizing income patterns. To exaggerate, both myself two years from now and Elon Musk would fall into this group! </p> <p></p> <p>We may reduce the variance further by splitting this group more narrowly or by removing outliers when conducting estimation.</p>"},{"location":"markdowns/Parameter_Estimataion/#theres-still-one-more-problem-sigma-is-unkown-here-comes-the-standard-error","title":"There's still one more problem - \\(\\sigma\\) is unkown. Here comes the Standard Error.","text":"<p>I believe many readers have heard of \"Standard Error.\" Now it\u2019s time to introduce what it is and why it\u2019s important. We just discussed why the \"Standard Deviation\" of the value we\u2019re trying to estimate (in our example, \\(\\bar{Y}\\), which is the average weight of elephants in the world) is relevant. According to the Central Limit Theorem, we know how to determine this value: \\(\\bar{Y} \\sim N\\left(\\mu, \\frac{\\sigma^2}{n}\\right)\\). However, the population variance, \\(\\sigma\\), is an unknown value. Therefore, we use the sample standard deviation as an estimator for the population standard deviation. $$ \\sigma_{\\bar{Y}} = \\frac{\\sigma}{\\sqrt{n}} \\approx s_{\\bar{Y}}= \\frac{s_{Y}}{\\sqrt{n}} $$ Therefore, \"Standard Error\" is an estimator of the \"Standard Deviation of the value we are trying to estimate.\"</p> <p>Remember:</p> <ol> <li>If we have a lower standard error, it means our prediction is likely more accurate.</li> <li>To achieve a smaller standard error, we need a larger N (sample size), but</li> <li>There are limitations if the \"Standard Deviation (or variance) of the population itself\" is large.</li> </ol> <p>Also, since \\( n \\) in the denominator of the standard error formula is square-rooted, as \\( n \\) increases, the impact of reducing standard error gradually diminishes.</p>"},{"location":"markdowns/Parameter_Estimataion/#gradually-diminishing-effect-of-increasing-n","title":"Gradually diminishing effect of increasing N","text":"<p>Let's try to plot this using entire Wage dataset. The following code plots how the variance of Wage changes, as the sample size increases:</p> <pre><code>sample_sizes &lt;- seq(10, 2910, by = 100)\n\nvariance_computation &lt;- function(s_size) {\n  temp_data &lt;- Wage[sample(nrow(Wage), s_size), ]\n  var(temp_data$wage)\n}\n\nvariances &lt;- sapply(sample_sizes, variance_computation)\n\nplot(sample_sizes, variances, main = 'Variance of Wage as sample size N increases', col = \"blue\", xlab = 'Sample size')\nlines(sample_sizes, variances, col = \"red\", lwd = 2)\n</code></pre> <p></p> <p>We can see that initially, the variance decreases quite dramatically as we increase the sample size. However, after reaching a sample size of 500, the improvement slows down and becomes negligible. This pattern resembles:</p> <p></p>"},{"location":"markdowns/Parameter_Estimataion/#coming-up-next-lets-look-into-simple-linear-regression-model","title":"Coming up next: let's look into 'Simple Linear Regression' model","text":"<p>Now we have a better understanding of the true implications of \"estimation\" and \"standard error.\" But I hear someone saying...</p>"},{"location":"markdowns/Parameter_Estimataion/#its-just-taking-average-for-estimation-its-not-a-real-modeling","title":"It's just taking average for estimation! It's not a REAL modeling!","text":"<p>However, the definition of 'modeling' is making a function, that takes input data, and output some estimation of target variable. So, mean estimator could be defined as: $$ \\hat{y} = f(x) = E[X] = \\frac{1}{n}\\sum_{i=1}^nx_i $$</p> <p>It could also be considered a type of model. BUT, I totally get what you mean. In the next post, I will use the example of Linear Regression to explore how Standard Error works in evaluating regression models and how we can properly utilize regression models for better prediction. (Remember, models are not magic boxes!)</p>"},{"location":"markdowns/Regression_Evaluation/","title":"\ud83d\udcc4 Regression_Evaluation","text":"<p>reference_docs: \"Parameter_Estimation, test_stat_basics\"</p> <p>This time, I will cover 'Linear Regression Model'. I know there are bunch of great resources on Linear Regression itself, so I will focus more on some of significant implications on Linear Regression, and some of the thoratical backgrounds behind methods we might have used without deeper understanding. Some of the questions answered would be:</p> <ul> <li>What does Linear Regression imply in terms of conditional prediction? (Why does it work better than mean estimation?)</li> <li>What are differences between just showing high correlation, and doing linear regression?</li> <li>What impacts accuracy of linear regression prediction? (Or, standard error?)</li> <li>Why do we do the log-scale? - Is it just because numbers are so large?</li> </ul> <p>Last time, we have identified:</p> <ol> <li>'Modeling' impies finding out any kind of functions, that can describe and predict 'real world patterns'</li> <li>Since it is impossible to get precise value of 'true patterns', we are using 'estimator' (e.g. Sample mean as an mean estimator, to estimate mean of population data.)</li> <li>'Standard Error' tell us about confidence interval of the 'true pattern' - e.g. even if we somehow identified 'true mean' of any elephants, if standard deviation is too large, mean would not help predicting future values 'with confidence'. </li> <li>So, knowing standard deviation is important, but since we can't not figure out 'population standard deviation (std of true patterns)' we utilize sample standard deviation : 'Standard Error'. Smaller standard error implies, narrower confidence interval - which means we can use estimator to predict future value with 'more confidence'.</li> </ol> <p>This time, let's look at one of the most widely used and important model - linear regression.</p>"},{"location":"markdowns/Regression_Evaluation/#linear-regression-model-what-is-the-correct-way-to-use-them","title":"Linear regression model - What is the 'correct' way to use them?","text":"<p>I think linear regression is one of the modeling method which is widely used, and many people will be used to it's concepts, formulation, or how to implement them in Python or R codes. However, it is more  important to implement them with better understanding. For any kind of data, that 'seems to have linear correlation' if we put them into the linear regression model, it gives some prediction. However, to what extent could we trust this outcome? What are the factors that impact accuracy of prediction based on linear regression?</p> <p>From one of the previous post on MLE (maximum likelihood estimator), we had looked into linear regression in terms of Bayesian approach.</p> <p>Recap: We can interpret 'Likelihood' as: given the observation or data, how likely is the data coming from certain model? For instand, when we toss the coin 10 times and see head 2 times, it is more likely to think probability of getting head is lower than 50%. Here, if we say \\(\\theta\\) is probability of getting head, \\(L(\\theta) = \\theta^x(1-\\theta)^{n-x}\\) where \\(x = 2\\). Likelihood will be maximized then \\(\\theta\\) is around 0.2. </p> <p>Like we see from the ilustration below, it is more likley to infer that the data below, are generated from 'true line' model A, rather than model B. The MLE approach tried to get a model which maximizes likelihood - and we found out that it is same as linear regression model with regularization. </p> <p></p> <p>This time, let's try to understand linear regresssion itself.</p>"},{"location":"markdowns/Regression_Evaluation/#simple-linear-regression-its-all-about-conditional-prediction","title":"Simple Linear regression - It's All About Conditional Prediction","text":"<p>I love using easy examples, so let's keep using example of elephants. I think many of people are used to illustrations like above. Linear regression is modeling pattern between two (or more) variables. However, it is also important to understand that it is 'expanding prediction of \\(Y\\) into conditional prediction given \\(X\\).'</p> <p>Let's remove the X axis, projecting data toward Y axisd. Now we only have sample statistic data of 'weight', so the best predictor for weight of 'all elephants in the world' would be a sample mean - this is exactly same as the mean estimation we'd done form the previous post. However, when additional information 'Age' is given (\\(X\\)), we can intuitively see that our model coule be more precise. Why? because based on their age elephants tend to have different weight, but mean prediction does not reflect this information. If we split up group of elephants by range of their age group, we can see that their 'group mean' moves. </p> <p></p> <p>So, what is the conclusion here? </p> <ul> <li>Linear regression is a conditional perdiction, predicting value of Y given that \\(X=x\\), where \\(x\\) is the value of data point.</li> <li>Also, we can recognize that 'linear regression' (or conditional prediction) is better than simple mean prediction when Y moves as X moves - which means, X and Y are somehow 'correlated'.</li> </ul>"},{"location":"markdowns/Regression_Evaluation/#quick-review-on-correlation-and-covariance","title":"Quick Review on Correlation and Covariance","text":"<p>Let's co back to statistic class for a momne. 'Covariance' measures how random variables X and Y moves together (increases, or decreases together.) $$ Cov(X, Y) = \\frac{\\sum_i^{n}(X_i - \\bar{X})(Y_i - \\bar{Y})}{n} $$ Have you thought of implication behind this formulation? Covariance is high when, for each of data points, value of x is far from the mean, value of your is also far from the mean. If deviation from mean is large for one variable, but small for the other variable, covariance will be canceled out. </p> <p>Look at the illustrations below. Two cases have same mean of X and Y, but only for the first case two variables are correlated. We could see that for the first case, as X deviates from the mean, Y also deviates from the mean. However, for the second case, for many data points Y value does not deviate from mean while X deviates from the mean.</p> <p> $$ Corr(X, Y) = \\frac{Cov(X, Y)}{\\sigma_X \\sigma_Y} $$ For sample correlation: $$ Corr(X, Y){sample} = \\frac{S{XY}}{s_X s_Y} $$</p> <p>Correlation is basically computed by dividing Covariance, with each random variable's standard deviation. By doing this, we are standardizing covariance between the value of -1 and 1. Correlation seems to be representing linear relationship between random variables in some sense, but there is one problem: Correlation is unitless, so we cannot use them for prediction of future value. For instance, if age and weight of elephants show high correlation of 0.8. What does this number 0.8 imply? It only shows 'how correlation is strong' as a relative metric, but does not tell us anything about 'how to predict weight, given age'. However, the correlation itself is related to the linear regression coefficient.</p>"},{"location":"markdowns/Regression_Evaluation/#linear-regression-and-correlation","title":"Linear regression and correlation","text":"<p>\\(Y = b_0 + b_1X\\)</p> <p>Now, let's look at the linear regression model in detail. We are fitting a line on dataset by representing Y as linear function of X. \\(b_0\\) and \\(b_1\\) are two parameters that decides how to draw the line. Based on given data, what we are doing here is same as what we have done from the mean estimation: </p> <ol> <li>Assuming there are linear 'true line' that generated the data</li> <li>Taking given data as sample, we are 'estimating' the 'true line' equation. </li> <li>If our estimated line seems to be appropriately accurate (which means, it shows small standard error), we can use this estimated line to predict unseen future value.</li> </ol> <p>Values we are estimating will be \\(b_0\\), and \\(b_1\\). From our given dataset, the best estimator should have least square error, as we also mentioned from this post. I will skip how does Least Square Error solution is derived, but Least Square Solution is known as: $$ b_1 = \\frac{\\sum_i^{N}(X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sum_i^{N}(X_i-\\bar{X})^2} = \\frac{s_{XY}}{s_X^2} $$ $$ b_0 = \\bar{Y} - b_1\\bar{X} $$</p> <p>Doen't this look familiar? This is sample correlation multiplied by \\(s_Y\\) over \\(s_X\\) $$ Corr(X, Y)_{sample} \\frac{s_Y}{s_X} $$ This implies that:</p> <ul> <li>The regression coefficient (slope) is, adjusting (scaling) correlation into the unit of Y: divided by STD of X and multiplied by STD of Y. </li> <li>In linear regression, \\(b_1 = 0\\) means two variables are not related, so we cannot predict Y with X. If correlation between X and Y is 0, this will make \\(b1\\) also 0. This explains intuition that, linear regression model will only be valid if variables are correlated.</li> </ul>"},{"location":"markdowns/Regression_Evaluation/#two-more-insights-on-residual-e-epsilon-and-lease-square-error-solution","title":"Two more insights on residual \\(e\\), \\(\\epsilon\\), and Lease Square Error solution.","text":""},{"location":"markdowns/Regression_Evaluation/#least-square-solution","title":"Least Square Solution","text":"<p>Least Square Solution is knows as:  $$ b_1 = \\frac{\\sum_i^{N}(X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sum_i^{N}(X_i-\\bar{X})^2} = \\frac{s_{XY}}{s_X^2} $$</p> <p>And, \\(b_0 = \\bar{Y} - b_1\\bar{X}\\)</p> <p>There are some more implication behind this. If we look at \\(b_0\\) formula, and plug it into \\(Y  = b_0 + b_1X\\), we can find out that this Least Square Solution must pass \\((\\bar{X}, \\bar{Y})\\).</p> <p></p>"},{"location":"markdowns/Regression_Evaluation/#more-abot-residual-and-error","title":"More abot residual, and error","text":"<p>Now, let's dive deep into \\(\\epsilon\\) and \\(e\\). </p> <p></p> <p>From the illustration above, let's say that the 'Blue line' is the 'ideal true linear line' which describes linear relatioship between age and weight of all elephants in the worlds. We are trying to 'predict' the line, from the given data. Here is a thing we need to keep in mind: 'True line' itslef also has 'irreducible error', and we define this as \\(\\epsilon\\).* </p> <p>Even we assume that the 'true line' is ideal line, it has error - unless all data points are exactly located on the true line. We call this 'irreducible error', because even we predict the true line precisely (which won't be possible, unless we have infinite data point) we cannot reduce this error. This is why we formulate 'true line as'</p> <p>\\(Y = \\beta_0 + \\beta_1X + \\epsilon\\), and our prediction as \\(\\hat{Y} = \\beta_0 + \\beta_1X + e\\). Here are two important insights for residual for 'least square estimator':</p> <ol> <li>Mean of residual should be 0. </li> <li>Correlation between \\(e\\) and \\(X\\) should be 0. (\\(Corr(e, X) = 0\\))</li> <li>Correlation between \\(\\hat{Y}\\) and \\(e\\) should be zero. (Since \\(\\hat{Y}\\) is dependent on \\(X\\), if 2 is true, this is also true. )</li> </ol> <p>The first one seems to be more intuitive: If the regression model has 'least square error', sum of it's errors will be canceled out to be zero. How about the second one? It implies 'Error should be consistent over entire range of X'. </p>"},{"location":"markdowns/Regression_Evaluation/#decomposition-of-error-term-sse-ssr-tss-and-r-square","title":"Decomposition of error term - SSE, SSR, TSS, and R Square","text":"<p>From the last post, we discussed that 'Variability' is important in measuring predictability (or accuracy) of the model (estimation). This is why we focus on variance, or standard error. So, let's look at how could 'Variability of Y' be decomposed.</p> <p>We know that:</p> <ul> <li>\\(\\sum_{i=1}^Ne_i = 0\\)</li> <li>\\(corr(e, X) = 0\\)</li> <li>\\(corr(\\hat{Y}, e) = 0\\)</li> </ul> <p>Therefore, we can write  $$ Var(Y) = Var(\\hat{Y} + e) =  Var(\\hat{Y}) + Var(e) + 2Cov(\\hat{Y}, e) = Var(\\hat{Y}) + Var(e) $$ Here, 'variability of Y' is decompsed with variability of \\(\\hat{Y}\\) and variability of \\(e\\). We discussed that 'error term' is related to 'irriducible error'. Therefore only the \\(Var(\\hat{Y})\\) is the variability related to the regression model we build - which, we can explain with our regression model. So, we may wish variability related to our regression will be higher that the irreducible error - cause that means our model explain huge part of the variances in \\(Y\\), and our model fits better.</p> <p>This is what 'R squre all about'</p>"},{"location":"markdowns/Regression_Evaluation/#why-r-square-is-not-accuracy-measure-but-measure-of-fit","title":"Why R Square is not 'accuracy measure', but measure of 'fit'?","text":"<p>It is easy to be too obsessed with R Square, and misinterpret it as 'accuracy measure'. However, if we search the definition, R Square is defined as a coefficient of determination. This is the definition from Wikipedia: R Square is the proportion of the variation in the dependent variable that is predictable from the independent variable(s)</p> <p>We had seen that \\(Var(Y) = Var(\\hat{Y}) + Var(e)\\). Since sample variance of a random variable is \\(\\frac{1}{n-1} \\sum (x_i - \\bar{x})\\), we can rewrite the formulation as: $$ \\sum_{i=1}^N(Y_i - \\bar{Y})^2 = \\sum_{i=1}^N(\\hat{Y_i} - \\bar{Y})^2 + \\sum_{i=1}^Ne_i^2 $$ These are the definition of TSS, SSR, SSE $$ TSS = SSR + SSE $$</p> \\[ R^2 = \\frac{SSR}{SST} = 1 - \\frac{SSE}{SST} \\] <p>Now, do you see the intention, and motivation behind R Square? </p> <ul> <li>R Square does not answer the question how is the model accurate?</li> <li>It answers: 'How much of variability is explained by the model?'</li> </ul> <p>This is about 'goodness of fit', not about accuracy of prediction.</p>"},{"location":"markdowns/Regression_Evaluation/#then-how-do-we-evaluate-accuracy-always-standard-error","title":"Then, how do we evaluate accuracy? - Always Standard Error.","text":"<p>So, how do we evaluate the model in terms of the accuracy of prediction? We had done very similar thing for the mean estimation from the previous post - Standard Error. Only the different thing is that, this time we have 3 parametors:</p> <ul> <li>\\(\\sigma_e\\): Standard error of regression. We need to estimate, how large is the 'standard deviation' of residuals are. </li> <li>\\(\\beta_0\\) and \\(\\beta1\\): We need to evaluate how is our estimated parameters distributed, and estimate their standard deviation through standard error of estimation. </li> </ul> <p>This is why I covered the concept of standard error in detail, with simple example of mean estimation from the previous post. Whatever we are estimating, what matters is 'standard deviation' - higher the standard deviation, it means that the 'range (interval)' that unseen future data could likely be located will be wider. For 95% confidence interval, 95% of data are located in estimated point \\(\\pm 2\\sigma\\), so larger sigma will cause wider interval. </p> <p>I won't go deep into the calculation details, but each of the standard errors could be induced as the following</p> <ul> <li> <p>\\(\\sigma_e\\): We use sample standard deviation instead:   $$   \\hat{\\sigma_\\epsilon}^2 = s^2 = \\frac{1}{N-2}\\sum_{i=1}^{N}e_i^2 = \\frac{SSE}{N-2}   $$</p> </li> <li> <p>\\(\\sigma_{\\beta_1}\\) We use sample standard deviation here too:    $$   \\hat{\\sigma_{\\beta_1{}}} = \\sqrt{\\frac{\\sigma_{\\epsilon}^2}{(N-1)s_x^2}} \\approx \\sqrt{Var(b_1)} = s_{b_1} = \\sqrt{\\frac{s^2}{(N-1)s_x^2}}   $$</p> </li> </ul> <p>Don't worry about the computation, when we do the regression analysis with Python, or R, it will compute and give all the numbers. However, the thing we need to know is 'Standard Error indicates how each of the estimated parameters are credible'</p> <ul> <li>Lower SE for slope \\(b_1\\) implies that, 'regression coefficient' for two variables are credible.</li> <li>Lower SE of regression means, if we make prediction based on the linear model, it is highly likely that actual value will be close enough to our predicted value!</li> </ul>"},{"location":"markdowns/Regression_Evaluation/#one-more-thing-what-matters-for-low-se-and-why-does-log-scale-works","title":"One More thing - What matters for low SE, and why does log-scale works?","text":"<p>For the standard error \\(\\hat{\\sigma_\\epsilon}^2\\), there is similar imlication we saw from mean estimation problem: As we have larger sample size, the standard error will diminish. How about standard error of the slope coefficient? $$ \\sqrt{\\frac{s_e^2}{(N-1)s_x^2}} $$</p> <ul> <li>Numerator: Smaller sample standard deviation of 'errors' will make standard error of the slope smaller. (This is not a standard deviation of the sample data, but standard deviation of sample 'error' - \\(\\hat{\\sigma_\\epsilon}^2\\))</li> </ul> <p>This is one of the reasons why log-scaling work. Since log-scaling compress the scale of data, it helps reducing variability, when the value is too spread out. Log-scaling mitigates skewness and transforms data to fit linear assumptions better.</p> <ul> <li>Denominator:</li> <li>Larger sample will make standard error of the slope smaller.</li> <li>Larger standard deviation of \\(x\\) will make standard error of the slope smaller.</li> </ul> <p>All of the above seems intuitive, except for one thing - the last one. Isn't it better to have 'smaller' variance of the data? </p> <p>However, larger variance in \\(x\\) implies having more 'various' data. If X covers more wider range of values, we have more information. </p>"},{"location":"markdowns/Regression_Evaluation/#wait-then-how-is-the-accuracy-of-prediction-with-this-model-like","title":"Wait, then how is the accuracy of 'prediction' with this model like?","text":"<p>Now we know how to evaluate our model, and what kind of factors impact these accuracies. However, the standard errors we had looked at are about 'errors' within the model - how far are data in general variating from the fitted line? How far is our predicted line, from the actual 'true line?'</p> <p>Then, how do we measure the accuracy of 'point estimation', if we are estimating unseen future Y value given some X value? </p>"},{"location":"markdowns/Regression_Evaluation/#sampling-errors","title":"Sampling Errors","text":"<p>When making prediction, we need to consider Sampling Error. This is because, our model only took 'sample' of data, not entire population (which is impossible) - so our estimator of \\(\\beta_0, \\beta_1\\) use sample values, and we need to make adjustment for this. We can decompose Prediction Error into 'sampling error' and irreducible error (from variaboility of Y, which is irrelevant to X. ): $$ e_f = \\epsilon_f - Error_{sampling} = Y_f - b_0 - b_1X_f $$ which could be decomposed as: $$  = \\epsilon_f + (\\beta_0 - \\beta_1X_f) - (b_0 - b_1X_f) $$ So, here the sampling error could be represented as: $$ ((b_0 - \\beta_0) + (b_1 - \\beta_1)X_f) $$ which is error between 'estimated model and true line' parameters. Let's leave computation to the computer. The standard error of point estimation, \\(Var(\\hat{Y})\\) is computed as the following: $$ S_{\\text{pred}} = s \\sqrt{1 + \\frac{1}{N} + \\frac{(X_f - \\bar{X})^2}{(N-1)s_X^2}} $$ Here, \\(s\\) is standard error of regression, which is \\(\\hat{\\sigma_{\\epsilon}}^2 = s^2 = \\frac{1}{N-2} \\sum_{i=1}^N e_i^2 = \\frac{\\text{SSE}}{N-2}\\)</p> <p>So, how is the standard error decomposed?</p> <ul> <li>\\(s * 1\\) which is variation of epsilon) is SE of regression, which is unrelated to X</li> <li>\\(\\frac{1}{N} + \\frac{(X_f - \\bar{X})^2}{(N-1)s_X^2}\\) is the part which is related to X</li> </ul> <p>Here are 3 implications, which are similar to all other standard errors we had looked into. </p> <ol> <li>Larger N matters</li> <li>Larger \\(S_x\\) matters (More variability of sample data X)</li> <li>As \\(X_f\\) is far from \\(\\bar{X}\\) \u2192 larger S_pred</li> </ol> <p>Only the last one is the new intuition. Drawing an illustration of the plot actually helps more intuitively understanding why:</p> <p></p> <p>Look at the green line, X1 and X2, X2 is far from mean of X, and we can see how point estimation in X2 is more far from true value, compared to X1! (Remember,  Least Square Error estimator should pass through mean of X, and Y.)</p> <p>Now we have covered what decides 'More Accurate' estimation. </p> <p>Based on these baselines, I will try to deal with Logistic Regression, and Multi-variate Regressions later.</p>"},{"location":"markdowns/test_MongoDB/","title":"\ud83d\udcc4 test_MongoDB","text":"<p>reference_docs: \"test_SQL\"</p>"},{"location":"markdowns/test_Redis/","title":"\ud83d\udcc4 test_Redis","text":"<p>reference_docs: \"test_SQL\", \"test_MongoDB\"</p>"},{"location":"markdowns/test_SQL/","title":"\ud83d\udcc4 test_SQL","text":"<p>reference_docs: \"\"</p>"},{"location":"markdowns/test_stat_basics/","title":"\ud83d\udcc4 test_stat_basics","text":"<p>reference_docs: \"</p>"},{"location":"markdowns/test_upload_omited_variables/","title":"\ud83d\udcc4 test_upload_omited_variables","text":"<p>reference_docs: \"Regression_Evaluation\", \"Parameter_Estimataion\", \"Dummy_test\"</p>"}]}