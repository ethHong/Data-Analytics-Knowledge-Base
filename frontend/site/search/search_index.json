{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to the MSBA Knowledge Base","text":"\u00d7                 Select a document to view."},{"location":"#network-based-wiki-for-business-analytics-and-data-professionals","title":"Network based Wiki for Business Analytics, and Data professionals","text":""},{"location":"contributors/","title":"Contributors","text":"Edit Contributions \u00d7              Refresh List            Save Changes Cancel"},{"location":"admin/","title":"Admin Dashboard","text":"<p>Welcome to the Admin Dashboard. Please select one of the following management sections:</p>"},{"location":"admin/#available-management-sections","title":"Available Management Sections","text":""},{"location":"admin/#document-management","title":"\ud83d\udcc4 Document Management","text":"<p>Manage all documents in the knowledge base, including: - View all available documents - Search through documents - Refresh document list</p>"},{"location":"admin/#contributor-management","title":"\ud83d\udc65 Contributor Management","text":"<p>Manage contributors to the knowledge base, including: - View all contributors - Add new contributors - Edit contributor information - Manage document contributions - Remove contributors</p> <p>category_specifier: \"Admin\" </p>"},{"location":"admin/contributors/","title":"Contributor Management","text":"Edit Contributions \u00d7 Cancel Save Changes Add New Contributor \u00d7 Name * Organization * LinkedIn URL Profile Image URL Leave empty to use auto-generated avatar Cancel Save"},{"location":"admin/documents/","title":"Document Management","text":"Refresh"},{"location":"markdowns/AB%20Testing%20Framework%20With%20Regression/","title":"AB Testing Framework With Regression","text":"<p>category_specifier : \"Causal Inference\"</p> <p>Reference Docs: Using Control Variables</p>"},{"location":"markdowns/AB%20Testing%20Framework%20With%20Regression/#contents","title":"Contents","text":""},{"location":"markdowns/Bass%20Model%20for%20Demand%20Prediction/","title":"Bass Model for Demand Prediction","text":"<p>category_specifier : \"Marketing Analytics\"</p> <p>Reference Docs: Linear Regression and Coefficient</p>"},{"location":"markdowns/Bass%20Model%20for%20Demand%20Prediction/#motivation","title":"Motivation","text":"<p>(Image source: Wikipedia)</p> <ul> <li>Demand over time (new adopted users) usually Increase \u2192 Peak \u2192 decrease (since number of population is limited)</li> <li>How can we estimate market demand of product, with given data?</li> <li>Objective: Assessing future demand, when will it peak and fall.</li> </ul>"},{"location":"markdowns/Bass%20Model%20for%20Demand%20Prediction/#framework","title":"Framework","text":""},{"location":"markdowns/Bass%20Model%20for%20Demand%20Prediction/#settings","title":"Settings","text":"\\[ M: \\text{Market Size} \\] \\[ N(t): \\text{Number of new customers at time t}  \\] \\[ A(t) = \\sum_{i=1}^{t-1}N(i): \\text{Accumulative `already customers' prior to time t }  \\] \\[ R(T) = M - A(t): \\text{Remaining customers at t} \\] \\[ p : \\text{Coefficient of innovation} \\] \\[ q: \\text{Coefficient of imitation} \\]"},{"location":"markdowns/Bass%20Model%20for%20Demand%20Prediction/#bass-model-is-motivated-by-hazard-rate","title":"Bass model is motivated by 'Hazard Rate'","text":"<ul> <li>Hazard rate: Probability of event occurring \u2018given that\u2019 the event not already occurred:   $$ H(t) = \\frac{N(t)}{R(t)}$$, where $$</li> <li>In marketing context: Probability of becoming a customer, given that they are not customer yet.</li> <li>Bass model assume this is equal to:   $$H(t)  = p + q \\frac{A(t)}{M} $$</li> <li>Interpretation : Innovation (customers make decision to use new product by themselves) + imitation (Imitate others' activity as product spread out) * ratio of already customers in the market. </li> </ul>"},{"location":"markdowns/Bass%20Model%20for%20Demand%20Prediction/#further-formulation","title":"Further Formulation","text":"\\[ H(t) = \\frac{N(t)}{R(t)}= p + q \\frac{A(t)}{M} \\] <ul> <li>Here, we can drive:</li> </ul> \\[ N(t) = H(t) \\times R(t) \\\\ = [p + q \\frac{A(t)}{M}] \\times R(t) \\\\ = Mp + (q-p)A(t) - \\frac{q}{M}[A(t)]^2 \\]"},{"location":"markdowns/Bass%20Model%20for%20Demand%20Prediction/#when-we-know-p-q-m-we-can-predict-the-demand-curve","title":"When we know p, q, M, we can predict the demand curve","text":"<ul> <li>In this model, if we know variables \\(M, p, q\\):</li> <li>\\(A(1) = 0, R(1) = M\\) \u2192 Get the Initial value at time \\(t\\)</li> <li>Get \\(H(t) = p + q \\frac{A(t)}{M}\\)</li> <li>Get \\(N(t) = H(t) * R(t)\\)<ul> <li>\\(N(1) = H(1) * R(1)\\)</li> </ul> </li> <li>Get \\(A(t+1)\\)<ul> <li>Iterate over time series \\(t\\) to get future demand curve: \\(A(2) = N(1) + A(1) = N(1)...\\)</li> </ul> </li> </ul>"},{"location":"markdowns/Bass%20Model%20for%20Demand%20Prediction/#takeaways","title":"Takeaways","text":""},{"location":"markdowns/Bass%20Model%20for%20Demand%20Prediction/#interpretation-of-the-curve","title":"Interpretation of the curve","text":"<ul> <li>Usually, \\(p &lt; q\\) (imitation effect is larger)</li> <li>Most people are reluctant to adopt new things. They want validation until others buy, and test products. (conservative)</li> <li>There are 2 exceptional categories on this:</li> <li>Value itself is evident (e.g. : Electricity over traditional lights)</li> <li>Risk itself is so small (low cost): App download.</li> <li>Most common shape of the curve : Increase \u2192 Peak \u2192 Decrease</li> </ul>"},{"location":"markdowns/Bass%20Model%20for%20Demand%20Prediction/#more-about-p-q","title":"More about \\(p, q\\)","text":"<ul> <li>\\(p\\) effects on earlier phase, \\(q\\) effects on later phase.</li> <li>Larger p: Peak of the curve become earlier (faster)</li> <li>Larger q: Reach to the peak faster (slope) and peak be higher</li> <li>They both impact slope (Speed of adoption)</li> </ul>"},{"location":"markdowns/Bass%20Model%20for%20Demand%20Prediction/#4-main-utilization-of-the-curve-for-marketing-decisions","title":"4 Main utilization of the curve for marketing decisions","text":"<ol> <li>How \u2018high\u2019 the curve peak will be</li> <li>How \u2018shallow\u2019 the curve is (Derivative. How fast growing it is)</li> <li>If curve is predicted to be shallow, outsourcing will be more effective.</li> <li>If the product is in Earlier / Later part of the curve (Innovator VS Imitators)</li> <li>If the curve is in early stage, target innovators. If not, target imitators.</li> <li>When will the curve end. (Prepare for the next generation product)</li> </ol> <p>\u2192 Decisions should be mostly made based on \\(R\\) (potential customers in market), not \\(M\\) (entire population in the market.)</p>"},{"location":"markdowns/Bass%20Model%20for%20Demand%20Prediction/#how-to-estimate-values-of-p-q-m","title":"How to estimate values of \\(p, q, M\\)","text":"<p>In bass model, p, q, M are unknown, and we need to predict those values</p> \\[ N(t) = Mp + (q-p) * A(t) - \\frac{q}{M}[A(t)]^2 \\] <ul> <li>The formulation above could be written as Linear Regression of variable \\(A(t)\\).</li> <li>Regress \\(N(t) ~ A(t)\\): \\(N(t) = a + bA(t) + cA(t)^2\\), by replacing \\(a = Mp, b = (q-p), c = -\\frac{q}{M}\\)</li> <li>After getting a, b, c, we can reorganize:</li> <li>\\(p = \\frac{\\sqrt{b^2-4ac}-b}{2}\\)</li> <li>\\(q = \\frac{\\sqrt{b^2-4ac}+b}{2}\\)</li> <li>\\(M = -q/c\\)</li> <li>After having \\(p, q, M\\) value, extrapolate future values, as explained in previous section.</li> </ul>"},{"location":"markdowns/Bass%20Model%20for%20Demand%20Prediction/#caution-do-not-get-m-from-the-regression-acquire-from-external-source","title":"Caution: Do not get \\(M\\) from the regression - acquire from external source","text":"<ul> <li>Bass model is very sensitive to errors: predicted curve will vary a lot given the data is impacted by variability.</li> <li>\\(M\\) is especially impacted a lot - \\(p, q\\) are not impacted as much as \\(M\\).</li> <li>Therefore, total market size \\(M\\) should better be estimated through external source - one example is using chain ratio method to estimate total market size.</li> </ul>"},{"location":"markdowns/Bass%20Model%20for%20Demand%20Prediction/#important-takeaway","title":"Important takeaway","text":"<p>When \\(M\\) is sourced from outside, it becomes a fixed number. Therefore, the equation if not linear anymore, so there is no closed for solution. </p> <p>Therefore, \\(p, q\\) value should be estimated using nonlinear regression techniques, with Python packages like Gradient based optimization method. </p>"},{"location":"markdowns/Difference%20in%20Difference/","title":"Difference in Difference","text":"<p>category_specifier : \"Causal Inference\"</p> <p>**Reference Docs: Fixed Effect</p>"},{"location":"markdowns/Difference%20in%20Difference/#motivation","title":"Motivation","text":"<p>\ud83d\udca1How can we estimate causal effects when we don\u2019t have an experiment? What if both treated and control groups are changing over time? </p>"},{"location":"markdowns/Difference%20in%20Difference/#when-does-diff-in-diff-apply","title":"When Does Diff-in-Diff Apply?","text":"<ul> <li>When there is a policy, treatment, or event that occurs at a specific time, affecting only some units.</li> <li>You observe two groups (treated vs. control) before and after the intervention.</li> <li>You suspect both groups would have followed similar trends if the treatment had not occurred.</li> </ul>"},{"location":"markdowns/Difference%20in%20Difference/#what-is-diff-in-diff","title":"What is Diff-in-Diff?","text":"<p>Difference-in-Differences is a quasi-experimental method that estimates causal effects by measuring how outcomes change over time between a treated group and a control group.</p> <p>The method accounts for both time-invariant differences between groups and common trends that affect both groups, thereby isolating the treatment's impact.</p> \\[ \\text{DiD Estimate} = (Y_{\\text{treated, after}} - Y_{\\text{treated, before}}) - (Y_{\\text{control, after}} - Y_{\\text{control, before}}) \\] <p></p>"},{"location":"markdowns/Difference%20in%20Difference/#why-it-matters","title":"Why It Matters","text":"<p>Diff-in-Diff is a powerful method to make credible causal claims without randomization. It allows us to account for:</p> <ul> <li>Baseline differences between treated and control groups</li> <li>Time trends that affect both groups equally</li> </ul> <p>This technique is widely used in policy analysis, economics, and business to evaluate the real impact of changes in the absence of experiments.</p>"},{"location":"markdowns/Difference%20in%20Difference/#how-to-apply-or-extend-diff-in-diff","title":"How to Apply or Extend Diff-in-Diff","text":"<p>Here are several ways to implement and strengthen DiD analysis:</p> <ul> <li>Parallel Trends Check: Visualize pre-treatment trends to justify the core assumption.</li> <li>DiD Regression Model: Estimate with interaction terms in a regression:</li> </ul> \\[ Y_{it} = \\alpha + \\beta_1 \\cdot \\text{Treated}_i + \\beta_2 \\cdot \\text{Post}_t + \\beta_3 \\cdot (\\text{Treated}_i \\times \\text{Post}_t) + \\epsilon_{it} \\] <ul> <li>\\(\\beta_3\\)  is the DiD estimator</li> <li>Two-Way Fixed Effects: Control for unit and time fixed effects in panel settings.</li> <li>Event Study Designs: Examine dynamic treatment effects over multiple periods.</li> <li>Placebo Tests: Apply DiD on fake treatment dates or untreated units to test robustness.</li> </ul>"},{"location":"markdowns/Difference%20in%20Difference/#examples","title":"Examples","text":"<ul> <li>Soda tax introduced in one city \u2192 compare beverage sales before and after with a nearby city.</li> <li>Company stops advertising on one platform \u2192 compare click-through rates (CTR) over time with another platform.</li> <li>State raises minimum wage \u2192 compare employment changes with a neighboring state that didn\u2019t.</li> </ul>"},{"location":"markdowns/Difference%20in%20Difference/#key-assumptions","title":"Key Assumptions","text":"<ul> <li>Parallel Trends: In the absence of treatment, the treated and control groups would have followed similar outcome trends.</li> <li>No Spillovers: The treatment does not affect the control group.</li> <li>No Other Confounding Events: Nothing else happened at the same time that only affected the treated group.</li> </ul>"},{"location":"markdowns/Difference%20in%20Difference/#key-equations","title":"Key Equations","text":"<p>DiD Regression Model:</p> \\[ Y_{it} = \\alpha + \\beta_1 \\cdot \\text{Treated}_i + \\beta_2 \\cdot \\text{Post}_t + \\beta_3 (\\text{Treated}_i \\times \\text{Post}_t) + \\epsilon_{it} \\] <ul> <li>\\(\\beta_3\\) gives the causal effect of the treatment.</li> </ul>"},{"location":"markdowns/Endogeneity%20and%20Exogeneity/","title":"Endogeneity and Exogeneity","text":"<p>category_specifier : \"Causal Inference\"</p> <p>**Reference Docs: Omitted Variable Bias | Statistical Bias | Instrument Variable</p>"},{"location":"markdowns/Endogeneity%20and%20Exogeneity/#motivation","title":"Motivation","text":"<ul> <li>Can I trust my regression to tell me about causality, or is something else interfering?</li> <li>Why does my coefficient change a lot when I add controls?</li> </ul> <p>If yes, X is exogenous \u2192 good for causal inference.</p> <p>If no, X is endogenous \u2192 need to fix it before interpreting causally.</p>"},{"location":"markdowns/Endogeneity%20and%20Exogeneity/#what-is-endogeneity","title":"What is Endogeneity?","text":"<p>Endogeneity occurs when an explanatory variable (X) is correlated with the error term (e) in a regression:</p> \\[ \\text{Cov}(X, e) \\neq 0 \\] <p>This violates a core assumption of OLS regression and causes biased and inconsistent estimates of causal effects.</p>"},{"location":"markdowns/Endogeneity%20and%20Exogeneity/#why-is-this-bad","title":"Why is this bad?","text":"<p>Because OLS will attribute part of the variation in Y to X, when in fact that variation is due to something else, leading to misleading results.</p>"},{"location":"markdowns/Endogeneity%20and%20Exogeneity/#example-scenarios","title":"Example Scenarios","text":"<ul> <li>Price \u2192 Quantity Demanded</li> </ul> <p>But prices are set in response to demand \u2192 simultaneity.</p> <ul> <li> <p>Education \u2192 Wages, but ability affects both \u2192 omitted variable.</p> </li> <li> <p>Health Insurance \u2192 Medical Spending, but only sick people get insurance \u2192 selection bias.</p> </li> </ul>"},{"location":"markdowns/Endogeneity%20and%20Exogeneity/#solutions-for-endogeneity","title":"Solutions for Endogeneity","text":"<ul> <li>Instrumental Variables (IV): Find an instrument Z that affects X but is uncorrelated with e.</li> <li>Randomized Experiments: Ensure X is randomly assigned.</li> <li>Panel Data / Fixed Effects: Remove time-invariant unobserved variables.</li> <li>Control Variables: Include omitted confounders if observable.</li> </ul>"},{"location":"markdowns/Endogeneity%20and%20Exogeneity/#what-is-exogeneity","title":"What is Exogeneity?","text":"<p>An explanatory variable (X) is exogenous when it is not correlated with the error term:</p> \\[ \\text{Cov}(X, e) = 0\\] <p>This means X is \"clean\" - any effect of X on Y is not confounded by unobserved variables.</p>"},{"location":"markdowns/Endogeneity%20and%20Exogeneity/#key-equations","title":"Key Equations","text":"<p>Core technical concepts:</p> <ul> <li>Exogeneity: \\(Cov(X,u)=0\\) or \\(E[u|X]=0\\)</li> <li>Endogeneity: \\(Cov(X,u)\\neq0\\)</li> </ul> <p>These conditions determine whether OLS estimates can be interpreted causally.</p>"},{"location":"markdowns/Endogeneity%20and%20Exogeneity/#endogeneity-vs-exogeneity","title":"Endogeneity vs. Exogeneity","text":"Endogeneity Exogeneity Cov (X, e) \u2260 0 (violation of OLS assumption) = 0 (satisfies OLS assumption) OLS Estimates Biased &amp; inconsistent Unbiased &amp; consistent Typical Causes OVB, simultaneity, measurement error Random assignment or natural shock Implication Can't interpret coefficients causally Can interpret coefficients causally Fixes IV, RCTs, controls, FE, panel methods No fix needed \u2014 already clean"},{"location":"markdowns/Fixed%20Effect/","title":"Fixed Effect","text":"<p>category_specifier : \"Causal Inference\"</p> <p>Reference Docs: Difference in Difference | Omitted Variable Bias</p>"},{"location":"markdowns/Fixed%20Effect/#motivation","title":"Motivation","text":"<p>Do I think unobserved, stable traits are driving both my treatment and outcome?</p> <p>If yes, and you have panel data - fixed effects is your friend.</p>"},{"location":"markdowns/Fixed%20Effect/#definition","title":"Definition","text":"<p>In panel data analysis, a Fixed Effects model gives each entity its own intercept term to capture time-invariant characteristics. It includes both unit fixed effects for entities and time fixed effects for period-specific factors.</p> <p>Fixed effects regression analyzes within-entity variation: how changes in an entity over time relate to changes in predictors. Each entity acts as its own control, accounting for permanent characteristics like a store's location or size.</p> <p>The model equation for two-way fixed effects is:</p> <p>\\(Y_{it} = \\alpha_i + \\lambda_t + \\beta X_{it} + \\epsilon_{it}\\)</p> <p>where \\(\u03b1_i\\) is the entity fixed effect and \u03bbt is the time fixed effect.</p> <p>Unlike random effects models, fixed effects don't assume entity differences are random. This approach controls for \"all unobserved heterogeneity that is constant over time\" without measuring it directly.</p>"},{"location":"markdowns/Fixed%20Effect/#why-it-matters-motivation","title":"Why It Matters / Motivation","text":"<p>Fixed effects help address omitted variable bias in panel data by controlling for unobserved characteristics that are constant over time without directly observing them.</p> <p>Fixed effects are crucial in difference-in-differences ( Difference in Difference analysis, allowing comparison of treated and control groups while accounting for baseline differences and time trends.</p> <p>Key benefits:</p> <ul> <li>They eliminate bias from unobserved, time-invariant factors, making our coefficient estimates closer to causal.</li> <li>They allow us to use within-unit variation, focusing on changes rather than levels.</li> <li>They handle panel data questions naturally by analyzing changes within an entity.</li> </ul>"},{"location":"markdowns/Fixed%20Effect/#examples","title":"Examples","text":"<ul> <li>Sales \u2192 Ad spending, but each store has its own baseline popularity \u2192 include store fixed effects.</li> <li>Test scores \u2192 Class size, but some students are naturally high-performing \u2192 use student fixed effects.</li> <li>Policy change \u2192 Employment, but each state has a fixed culture of regulation \u2192 include state fixed effects.</li> </ul>"},{"location":"markdowns/Fixed%20Effect/#how-fixed-effects-are-implemented-and-interpreted","title":"How Fixed Effects Are Implemented and Interpreted","text":"<p>Fixed effects models use dummy variables for each entity and time period (minus one reference).</p> <p>In interpretation, coefficients show the effect of a variable on Y while controlling for time-invariant entity differences. For example, in a store fixed effects model, a price coefficient shows how changes in price affect sales within the same store over time.</p> <p>Key considerations include degrees of freedom (N-1 dummies for N entities) and within R-squared vs overall R-squared. While fixed effects control for time-invariant factors, they don't guarantee causality - time-varying confounders may still exist.</p>"},{"location":"markdowns/Fixed%20Effect/#who-uses-fixed-effects","title":"Who Uses Fixed Effects","text":"<ul> <li>Economists/Academics: Used extensively in empirical research, especially for wage gaps and health outcomes</li> <li>Policy Analysts: Evaluate program effects across regions over time</li> <li>Data Scientists: Analyze user behavior, A/B tests, and customer patterns</li> <li>Financial Analysts: Study firm performance and market trends</li> <li>Educational Researchers: Control for school-level differences in student outcomes</li> </ul> <p>Fixed effects methods may appear under different names in industry, such as \"categorical embeddings\" in machine learning or \"entity encoders\" in deep learning.</p>"},{"location":"markdowns/Frisch%20Waugh%20Theorem/","title":"Frisch Waugh Theorem","text":"<p>category_specifier:  \"Causal Inference\"</p> <p>Reference Docs: Using Control Variables|Omitted Variable Bias (OVB)</p>"},{"location":"markdowns/Frisch%20Waugh%20Theorem/#motivation","title":"Motivation","text":"<ul> <li>When there is OVB (Omitted Variable Bias (OVB)) in linear model for an experiment (or observational data for treatment effect, and want to get unbiased estimator of this treatment effect, withouth regressing with entire variabled.</li> </ul>"},{"location":"markdowns/Frisch%20Waugh%20Theorem/#framework","title":"Framework","text":"<p>\ud83d\udca1When you know that the treatment is not randomizes (or partially randomized because it assigned experiment based on some characteristic, like demographic), we can use FW to control bias.</p> <ul> <li>You did not have experiment, and do have observation data</li> <li>You assume that the following equation is true line, while \\(X_1\\) is treatment variable (0, 1), \\(X_2\\) is control variable.</li> </ul> \\[ y =\\beta_0 + \\beta_1X_1 + \\beta_2X_2 + e \\] <ul> <li>Follow the 2-step regression below:<ol> <li>Regress treatment on other control variables:  <ul> <li>\\(X_1 = \\alpha_0 + \\alpha_1X_2 + \\tilde{X_1}\\)</li> <li>Implication of \\(\\tilde{X_1}\\): Part \\(X_1\\) which is not explained by \\(X_2\\)</li> </ul> </li> <li>Regress \\(y\\) on \\(\\tilde{X_1}\\)<ul> <li>\\(y = \\tilde{\\beta_0} + \\tilde{\\beta_1}\\tilde{X_1} + v\\)</li> </ul> </li> <li>then , \\(\\tilde{\\beta_1} = \\beta_1\\)</li> </ol> </li> </ul>"},{"location":"markdowns/Hypothesis%20testing/","title":"Hypothesis Testing","text":"<p>category_specifier : \"Statistics\"</p> <p>Reference Docs: Linear Regression and Coefficient|Omitted Variable Bias | Standard Error</p>"},{"location":"markdowns/Hypothesis%20testing/#motivation","title":"Motivation","text":"<ul> <li>We want to test (or check) if our estimator for some parameter is significant</li> </ul>"},{"location":"markdowns/Hypothesis%20testing/#use-cases","title":"Use cases","text":"<ul> <li>We want to check if mean of two groups are significantly different</li> </ul> <p>Application: Is metric difference in two user group significantly different?</p> <ul> <li>We want to check if regression coefficient is statistically significant</li> </ul> <p>Application:</p> <ul> <li>Prediction model context: Checking significance of the model coefficients.</li> <li>A/B testing (causal relationship) context: Checking if treatment effect coefficient is significant.</li> </ul>"},{"location":"markdowns/Hypothesis%20testing/#framework","title":"Framework","text":""},{"location":"markdowns/Hypothesis%20testing/#hypothesis-setup","title":"Hypothesis setup","text":"<ul> <li>We have estimated value for the true value \\(\\beta_1\\), as  \\(\\beta_{null}\\)</li> <li>We are checking if  \\(\\beta_1\\), and  \\(\\hat{\\beta_1}\\) are close enough (Similar)</li> </ul> \\[ H0 \\text{ (Null Hypothesis)}: \\beta_1 = \\beta_{null} , \\\\ \\text{or,} \\quad \\beta_1 - \\beta_{null} = 0 \\] \\[ H1 \\text{ (Alternative Hypothesis)}: \\beta_1 \\neq \\beta_{null}, \\\\ \\text{or,} \\quad \\beta_1 - \\beta_{null} \\neq 0 \\] <ul> <li>Usually, when we are checking if treatment effect is significant (if \\(\\beta_1\\) is not 0), we set  \\(\\beta_{null} = 0\\)</li> </ul>"},{"location":"markdowns/Hypothesis%20testing/#interpretation","title":"Interpretation","text":"<ul> <li>Since we don\u2019t have enough clue (backup) to believe  \\(\\beta_1\\) value is significant, we start by assuming H0 (Null Hypothesis).</li> <li>If there are significant level of clue to believe  \\(\\beta_1\\) is not 0, then we reject null and take H1 (Alternative hypothesis)</li> </ul>"},{"location":"markdowns/Hypothesis%20testing/#get-t-statistic-and-run-t-test","title":"Get t-statistic and run T-test","text":"\\[ t = \\frac{\\beta_1 - \\beta_{null}}{s_{\\beta_1}} \\] <ul> <li>Implication: Normalize gap between true value (\\(\\beta_1\\)), and hypothesized value (\\(\\beta_{null}\\))</li> <li>Interpretation:</li> <li>If \\(t\\) is small, (or close to 0), it means hypothesized value (\\(\\beta_{null}\\), or 0) is close to the true value. This means we have enough clue to take H0  (\\(\\beta_1\\) is close to 0)</li> <li>If  \\(t\\)  is large enough, it means \\(\\beta_1\\) is not likely to be 0, so we could reject null.</li> </ul>"},{"location":"markdowns/Hypothesis%20testing/#significance-level-deciding-if-t-is-large-enough","title":"Significance level - deciding if \\(t\\) is \u2018large enough\u2019","text":"<ol> <li>Pick Significance Level \\(\\alpha\\): Probability of rejecting Null when it\u2019s true</li> <li>Get a critical value: \\(t^*_{N-2, \\alpha/2}\\) (1.96 when \\(\\alpha\\) = 95%)</li> <li>Check if \\(t\\) value deviates critical value range: If \\(t = |\\frac{b_1 - \\beta^1}{s_{b1}}| &gt;t^*_{N-2, \\alpha/2}\\), then \\(t\\) Is \u2018large enough\u2019** so we can reject null.</li> </ol>"},{"location":"markdowns/Hypothesis%20testing/#p-value-how-small-t-should-be","title":"P-Value: \u2018How small\u2019 T should be?","text":"<ul> <li>P value: \\(p = Pr(| t^*_{N-2, \\alpha/2}| \\geq |t|)\\)</li> <li>Probability of \\(t\\) being smaller than critical value</li> <li>Probability of rejecting null, when \\(t\\) Is large enough</li> <li>Conclusion: Smaller P-value implies, \\(t\\) is more likely to be large.</li> </ul>"},{"location":"markdowns/Hypothesis%20testing/#rule-of-thumb-p-value-005","title":"Rule of Thumb : P value &lt; 0.05","text":"<ul> <li>In many cases we take 95% of critical value</li> <li>Under 95% critical value, we can reject null under P value &lt; 0.05</li> </ul>"},{"location":"markdowns/Hypothesis%20testing/#application","title":"Application","text":""},{"location":"markdowns/Hypothesis%20testing/#regression-coefficient-significance","title":"Regression coefficient significance","text":""},{"location":"markdowns/Hypothesis%20testing/#setting","title":"Setting","text":"<p>Assume we have estimated coefficient for Linear Regression (Linear Regression and Coefficient</p> <p>Where true line is:</p> \\[ Y = \u03b2\u2080 + \u03b2\u2081X + \u03b5 \\] <p>Estimated model is:</p> \\[ Y = \\hat{\u03b2\u2080} + \\hat{\u03b2\u2081X} + \\hat{\u03b5} \\]"},{"location":"markdowns/Hypothesis%20testing/#hypothesis-setting","title":"Hypothesis setting","text":"<ul> <li>We want to know if X has significant effect on Y based on the given data</li> <li>Therefore, we hypothesize if true value of \\(\\beta_1 = \\hat{\\beta_1}= 0\\), so here \\(\\beta_{null} =0\\)</li> </ul> \\[ Y = \\hat{\u03b2\u2080} + \\hat{\u03b2\u2081X} + \\hat{\u03b5} \\] \\[ H0 \\text{ (Null Hypothesis)}: \\beta_1 = \\beta_{null} , \\\\ \\text{or,} \\quad \\beta_1 - \\beta_{null} = 0 \\] \\[ H1 \\text{ (Alternative Hypothesis)}: \\beta_1 \\neq \\beta_{null}, \\\\ \\text{or,} \\quad \\beta_1 - \\beta_{null} \\neq 0 \\]"},{"location":"markdowns/Hypothesis%20testing/#t-test","title":"T-test","text":"<ul> <li>We can get t-statistic value from the trained linear model.</li> </ul> \\[ t = \\frac{\\beta_1}{s_{\\beta_1}} \\] <ul> <li>Compute P-value, and check if p-value is lower than 0.05</li> </ul>"},{"location":"markdowns/Hypothesis%20testing/#interpretation_1","title":"Interpretation","text":"<ul> <li> <p>If p-value for the coefficient is lower than 0.05, we can say the coefficient value is \u2018statistically significant\u2019.</p> </li> <li> <p>If the coefficient is not significant, we cannot trust the coefficient value, we cause we lack ground to say \\(\\beta_1 \\neq 0\\).</p> </li> <li> <p>In this case, we say the precision if coefficient is low, (High standard error Standard Error). To increase precision:</p> </li> <li> <p>Increase sample size \\(N\\)</p> </li> <li>Guarantee variation of the sample data \\(X\\) (\\(Var(X)\\))</li> <li>Take more control variables (Using control variables ) / features into regression, to reduce variance of \\(e\\) ( \\(s^2\\) )</li> </ul>"},{"location":"markdowns/Hypothesis%20testing/#comparison-of-two-groups-statistical-value-tbd","title":"Comparison of two group\u2019s statistical value (TBD)","text":""},{"location":"markdowns/Hypothesis%20testing/#takeaways-and-important-points","title":"Takeaways and Important points.","text":"<ul> <li> <p>When we reject null, we can say value of  \\(\\beta_1\\) is statistically significant, and trust the estimated coefficient value.</p> </li> <li> <p>Caution: It is not appropriate to say \u2018True</p> </li> </ul> <p>\\(\\beta_1\\) value is close to 0\u2019 when we don\u2019t reject null</p> <ul> <li>More appropriate interpretation: \u2018We cannot say \\(\\beta_1\\) is not 0 based on the current data, so cannot trust the coefficient. \u2019</li> </ul>"},{"location":"markdowns/Instrument%20Variable/","title":"Instrument Variable","text":"<p>category_specifier : \"Causal Inference\"</p> <p>Reference Docs: Omitted Variable Bias | Endogeneity and Exogeneity</p>"},{"location":"markdowns/Instrument%20Variable/#motivation","title":"Motivation","text":"<p>\ud83d\udca1I can\u2019t run an experiment, but I want to know the causal effect.</p> <ul> <li>You lack a clean A/B test or random assignment.</li> <li>You suspect non-random selection into treatment.</li> <li>IV is a method born out of necessity - when experiments aren\u2019t feasible and OLS is unreliable, IV becomes a powerful tool for credible causal inference.</li> </ul>"},{"location":"markdowns/Instrument%20Variable/#the-iv-solution","title":"The IV Solution","text":"<p>IV introduces a third variable (Z), the instrument, which:</p> <ol> <li>Is correlated with X (relevance) :</li> </ol> \\[  Cov(Z, X) \u2260 0 \\] <ol> <li>Is not directly correlated with Y (exogeneity) :</li> </ol> \\[ Cov(Z, e) = 0 \\] <p>This allows you to use only the part of X that is \u201cas good as random\u201d (driven by Z), isolating a causal estimate of \\(\\beta_1\\).</p>"},{"location":"markdowns/Instrument%20Variable/#when-do-we-use-iv","title":"When Do We Use IV?","text":"<p>Use IV when you suspect the explanatory variable (X) is correlated with the error term (e) in your model:</p> \\[ Y = \\beta_0 + \\beta_1 X + e \\quad \\text{where } \\text{Cov}(X, e) \\neq 0 \\] <p>This breaks the OLS assumption and leads to biased estimates.</p>"},{"location":"markdowns/Instrument%20Variable/#visual-representation","title":"Visual Representation","text":"<p>In a causal graph:</p> <p></p> <ul> <li>Z \u2192 X \u2192 Y</li> <li>Z \u2192 X must exist (relevance)</li> <li>Z \u2192 Y directly (exogeneity)</li> </ul>"},{"location":"markdowns/Instrument%20Variable/#tsls-two-stage-least-squares","title":"TSLS: Two-Stage Least Squares","text":"<p>A common IV estimation technique:</p> <ol> <li>First Stage:</li> </ol> <p>Regress X on Z:</p> <p>\\(X = \\pi_0 + \\pi_1 Z + u\\)</p> <p>Use predicted \\(\\hat{X}\\)</p> <ol> <li>Second Stage:</li> </ol> <p>Regress Y on \\(\\hat{X}\\)</p> <p>\\(Y = \\beta_0 + \\beta_1 \\hat{X} + \\epsilon\\)</p> <p>The estimated \\(\\beta_1\\) is now unbiased, assuming instrument validity.</p>"},{"location":"markdowns/Instrument%20Variable/#examples-use-cases","title":"Examples &amp; Use Cases","text":"<ul> <li>Demand estimation: Use weather shocks or tax variation to instrument for prices.</li> <li>Crime and incarceration: Use prison overcrowding litigation as an instrument.</li> <li>Class size and student performance: Use earthquake-driven classroom reallocations.</li> <li>Birth weight and smoking: Use state-level cigarette prices as an instrument for maternal smoking.</li> </ul>"},{"location":"markdowns/Instrument%20Variable/#testing-for-validity","title":"Testing for Validity","text":"<ul> <li>Relevance: Look at F-stat in the first stage. Rule of thumb: F &gt; 10 is strong.</li> <li>Exogeneity: Testable only if you have more instruments than endogenous variables using the J-test (Sargan test).</li> </ul>"},{"location":"markdowns/Instrument%20Variable/#generalizations","title":"Generalizations","text":"<ul> <li>Can include multiple instruments and control variables.</li> <li>Must ensure the model is at least exactly identified (number of instruments \u2265 number of endogenous regressors).</li> <li>Weak instruments bias TSLS toward OLS - use with caution!</li> </ul>"},{"location":"markdowns/Instrument%20Variable/#software-notes","title":"Software Notes","text":"<ul> <li>In Python: Use <code>IV2SLS</code> from <code>linearmodels.iv</code></li> <li>In R: Use <code>ivreg</code> from the <code>AER</code> package</li> </ul>"},{"location":"markdowns/Linear%20Regression%20and%20Coefficient/","title":"Linear Regression and Coefficient","text":"<p>category_specifier : \"Statistics\"</p> <p>Reference Docs: Hypothesis Testing | Using Control Variables | Prediction using Linear Regression</p>"},{"location":"markdowns/Linear%20Regression%20and%20Coefficient/#contents","title":"Contents","text":""},{"location":"markdowns/Omitted%20Variable%20Bias/","title":"Omitted Variable Bias","text":"<p>category_specifier : \"Statistics\"</p> <p>Reference Docs: Statistical Bias | Endogeneity and Exogeneity | Frisch Waugh Theorem | Instrument Variable | Hypothesis Testing | Using control variables</p>"},{"location":"markdowns/Omitted%20Variable%20Bias/#motivation","title":"Motivation","text":"<p>\ud83d\udca1What happens if we forget to control for something important? Is there a third variable that affects both X and Y?</p>"},{"location":"markdowns/Omitted%20Variable%20Bias/#when-does-ovb-arise","title":"When does OVB arise?","text":"<ul> <li> <p>When a relevant variable is:</p> </li> <li> <p>Omitted from the regression, and</p> </li> <li>Correlated with both the explanatory variable X and the outcome Y where Z is the omitted variable.</li> </ul> \\[ \\text{Bias} = \\text{Cov}(X, Z) \\cdot \\text{Effect of Z on Y} \\]"},{"location":"markdowns/Omitted%20Variable%20Bias/#definition","title":"Definition","text":"<p>Omitted Variable Bias (OVB) is a form of statistical bias that occurs when a key variable is excluded from a regression model, causing its effect to be wrongly attributed to other variables.</p>"},{"location":"markdowns/Omitted%20Variable%20Bias/#why-it-matters","title":"Why It Matters","text":"<p>OVB compromises our ability to identify true cause-and-effect relationships. When we omit a confounding variable, we risk drawing false conclusions about relationships between variables. This can lead to incorrect decisions in business and analytics. Understanding OVB reminds us that correlation is not causation and helps us identify potential hidden factors affecting our analysis.</p>"},{"location":"markdowns/Omitted%20Variable%20Bias/#how-to-address-ovb","title":"How to Address OVB","text":"<p>Here are four ways to address omitted variable bias:</p> <ul> <li>Add Controls: Include omitted variables in the regression.</li> <li>Fixed Effects: Control for time-invariant or entity-invariant factors using panel data.</li> <li>Instrumental Variables: Use variables that affect X but not the error term.</li> <li>Randomized Trials: Use A/B tests to ensure treatment independence.</li> </ul> <p>Combining these methods often works best to make variables more exogenous.</p>"},{"location":"markdowns/Omitted%20Variable%20Bias/#examples","title":"Examples","text":"<ul> <li>Education \u2192 Income, but we omit ability \u2192 upward bias.</li> <li>Ad spending \u2192 Sales, but we omit seasonality \u2192 spurious correlation.</li> <li>Police presence \u2192 Crime rates, but we omit neighborhood risk \u2192 distorted effect.</li> </ul>"},{"location":"markdowns/Omitted%20Variable%20Bias/#key-equations","title":"Key Equations","text":"<p>The bias in a coefficient estimate when omitting variable Z is:</p> \\[ Bias(\\tilde{\\beta}_1) \\approx \\beta_2 \\cdot \\frac{Cov(X,Z)}{Var(X)} \\] <p>This shows bias depends on Z's effect (\\(\\beta_2\\)) and its correlation with X. The exogeneity assumption \\(E[u|X]=0\\) is violated when important variables are omitted.</p>"},{"location":"markdowns/Standard%20Error/","title":"Standard Error","text":"<p>category_specifier : \"Statistics\"</p> <p>Reference Docs: Hypothesis Testing|Using Control Variables</p>"},{"location":"markdowns/Standard%20Error/#context","title":"Context","text":"<ul> <li>In A/B test, we want to improve the precision of the estimator for treatment effect (coefficient of the treatment variable)</li> </ul> <p>\u2192 Understand what affects the precision</p> <ul> <li> <p>Standard Error is useful in understanding how reliable our sample statistic is as an estimator of the population parameter</p> </li> <li> <p>Used to construct a confidence interval, which provides margin of error in our estimator</p> </li> <li>Used in the Hypothesis Testing</li> </ul>"},{"location":"markdowns/Standard%20Error/#definition","title":"Definition","text":"<ul> <li>Standard error measures standard deviation of the sampling distribution of a statistic (such as mean)</li> <li>It explains the accuracy of a sample statistic in representing the population parameter</li> <li>Lower standard error \u2192 More precise sample statistic</li> </ul> <p>Example: Standard Error of the Mean (SEM)</p> \\[ SEM = \\frac{s}{\\sqrt{n}} \\] <ul> <li>Measures the standard deviation of sample means around the true population mean</li> <li>Relies on the two factors: 1) Sample Size 2) Sample Standard Deviation</li> </ul>"},{"location":"markdowns/Standard%20Error/#application","title":"Application","text":""},{"location":"markdowns/Standard%20Error/#ab-test-estimate-the-true-treatment-effect-using-standard-error-of-coefficient","title":"A/B Test: Estimate the true treatment effect using standard error of coefficient","text":"<p>Standard Error of the Regression Coefficient</p> \\[ \\widehat{SE}(\\hat{\\beta}_1) = \\sqrt{\\frac{s^2}{\\sum_{i=1}^n (x_i - \\bar{x})^2}} = \\sqrt{\\frac{s^2}{(N-1)s_x^2}} \\] <ul> <li>Measures the variability of the estimated regression coefficients if we were to repeatedly sample and run regressions</li> <li>It explains how close the estimated treatment effect (reflected in the regression coefficient) is to the true treatment effect</li> <li>Relies on the three factors:</li> <li>To improve the precision of the coefficient estimator, we control these factors<ol> <li>\\(N\\) : Sample Size</li> <li>\\(s_x^2\\) : Variability in X</li> <li>\\(s^2\\) : Variance of regression residual</li> </ol> </li> </ul>"},{"location":"markdowns/Statistical%20Bias/","title":"Statistical Bias","text":"<p>category_specifier : \"Statistics\"</p> <p>Reference Docs: Endogeneity and Exogeneity | Omitted Variable Bias</p>"},{"location":"markdowns/Statistical%20Bias/#motivation","title":"Motivation","text":"<ul> <li>Am I missing variables that affect both X and Y? (Omitted Variable Bias)</li> <li>Am I measuring my variables accurately? (Measurement Error)</li> <li>Is my sample representative of the population? (Selection Bias)</li> <li>Is treatment assigned based on factors that also affect the outcome? (Endogeneity) </li> </ul>"},{"location":"markdowns/Statistical%20Bias/#definition","title":"Definition","text":"<p>Statistical bias is the systematic difference between the expected value of an estimator and the true value of the parameter it\u2019s trying to estimate.</p> \\[\\text{Bias}(\\hat{\\theta}) = \\mathbb{E}[\\hat{\\theta}] - \\theta\\] <p>Where:</p> <ul> <li>\\(\\hat{\\theta}\\) is your estimator (ex - sample mean, regression coefficient)</li> <li>\\(\\theta\\) is the true population parameter</li> </ul> <p>If the expected value of your estimator is not equal to the true value, the estimator is biased.</p>"},{"location":"markdowns/Statistical%20Bias/#why-does-it-matter","title":"Why does it matter?","text":"<p>Bias affects result reliability in data science. Ignoring it leads to flawed decisions in business, economics, and research.</p>"},{"location":"markdowns/Statistical%20Bias/#types","title":"Types","text":""},{"location":"markdowns/Statistical%20Bias/#selection-bias","title":"Selection Bias","text":"<p>Non-representative sampling of data, such as analyzing only satisfied customers.</p>"},{"location":"markdowns/Statistical%20Bias/#omitted-variable-bias","title":"Omitted Variable Bias","text":"<p>Excluding important variables from analysis, like studying education's impact on earnings without considering ability.</p>"},{"location":"markdowns/Statistical%20Bias/#measurement-bias","title":"Measurement Bias","text":"<p>Systematic errors in data collection from faulty instruments or biased questions.</p>"},{"location":"markdowns/Statistical%20Bias/#how-to-mitigate","title":"How to Mitigate","text":"<ul> <li>Randomization: Use random sampling and A/B tests</li> <li>Including Controls: Add relevant variables to models</li> <li>Proper Instrumentation: Use accurate measurement tools</li> <li>Awareness &amp; Training: Educate analysts on bias</li> <li>Comprehensive Data: Include both successes and failures</li> </ul>"},{"location":"markdowns/Using%20Control%20Variables/","title":"Using Control Variables","text":"<p>category_specifier : \"Causal Inference\"</p> <p>Reference Docs: Frisch Waugh Theorem | AB Testing Framework With Regression | Omitted Variable Bias | Standard Error | Linear Regression and Coefficient</p>"},{"location":"markdowns/Using%20Control%20Variables/#motivation","title":"Motivation","text":""},{"location":"markdowns/Using%20Control%20Variables/#in-randomized-ab-test","title":"In randomized AB test","text":"<p>\ud83d\udca1What should we do if, significance is low for A/B test?</p> <ul> <li>Adding control variable is one of three ways to help improving precision of AB Testing Framework With Regression (Lowering SE of regression coefficient estimate \\(\\beta\\))</li> <li>Choosing 50:50 for probability of control, treatment to maximize \\(Var(X)\\)</li> <li>Choosing right sample size to lower Standard Error</li> <li>Adding control variables in AB test</li> </ul>"},{"location":"markdowns/Using%20Control%20Variables/#in-observational-data","title":"In observational data","text":"<ul> <li>If you don\u2019t, or can\u2019t run random experiment, you cannot make causal interpretation in observational data</li> <li>How can we make the best causal claim from observational data, if you cannot run experiment? </li> <li>Adding control variables can isolate random variation, and remove \u2018non random\u2019 part of each variable. This can control some extent of engogeneity.</li> <li>Adding control variable can help controlling Omitted Variable Bias</li> </ul>"},{"location":"markdowns/Using%20Control%20Variables/#framework","title":"Framework","text":""},{"location":"markdowns/Using%20Control%20Variables/#if-you-have-run-random-ab-test","title":"If you have run random A/B test","text":""},{"location":"markdowns/Using%20Control%20Variables/#settings","title":"Settings","text":"<ul> <li>Consider univariate regression for a A/B test</li> </ul> \\[ Target = \\beta_0 + \\beta_1 * treatment + e \\] <ul> <li>Then, consider multivariate regression for the same A/B test.</li> </ul> \\[ Target = \\gamma_0 + \\gamma_1 * treatment + \\gamma_2X_2 + \\gamma_3X_3 + ... + u \\] <ul> <li>\\(Target\\) could be target variable as sales, while \\(X_2, X_3 ...\\) could be all other available variables (data we have: e.g. )</li> </ul>"},{"location":"markdowns/Using%20Control%20Variables/#takeaways","title":"Takeaways","text":"<ul> <li>Here, \\(Var(\\beta_1) \\geq Var(\\gamma_1)\\) always holds, which means adding up control variable can reduce error (improve precision) of the A/B test coefficients.</li> <li>CAUTION: For randomized AB test, we are not adding control variables for Omitted Variable Bias  - there is no OVB if we have randomized AB test.</li> </ul>"},{"location":"markdowns/Using%20Control%20Variables/#if-you-only-have-observational-data","title":"If you only have observational data","text":""},{"location":"markdowns/Using%20Control%20Variables/#settings_1","title":"Settings","text":"\\[ y =\\beta_0 + \\beta_1 * treatment+ e \\] <ul> <li>You don\u2019t have AB test results, but only observational data which is likely to have Omitted Variable Bias), because assignment of treatment is not random for each of the variables.</li> <li>Here, \\(\\beta_1\\) represents effect of the treatment, while \\(e\\) contains all the variation not explained by the treatment.</li> </ul> \\[ y =\\beta_0' + \\beta_1' * treatment  + \\beta_2X_2 +e' \\] <ul> <li>By adding omitted variable \\(X_2\\) as control, \\(\\beta_1'\\) will be less biased,</li> </ul> <p>removing bias caused by relationship between treatment and \\(X_2.\\)</p> <ul> <li>Since \\(OVB = \\beta_2 * \\frac{Cov(treatment, X_2)}{Var(X_2)}\\) (Omitted Variable Bias)), if \\(Cov(treatment, X_2)\\) is not 0, \\(\\beta_1'\\) will change.</li> <li>Since treatment is not randomly assigned regarding \\(X_2\\), \\(Cov(treatment, X_2)\\) is not likely to be 0</li> </ul>"},{"location":"markdowns/Using%20Control%20Variables/#takeaways_1","title":"Takeaways","text":"<ul> <li>\\(\\tilde{X_1}\\) represents the portion of \\(X_1\\)(treatment) that is not correlated with other variables.</li> <li>This allows us to estimate the true random treatment effect more accurately.</li> <li>By adding a control variable, we isolate the random effect of \\(X_1\\)removing confounding influences.</li> <li>Specifically, controlling for \\(X_2\\) ensures that the variation in \\(X_1\\) used for estimation is uncorrelated with \\(X_2\\) , leading to an unbiased estimate of the treatment effect.</li> </ul>"}]}